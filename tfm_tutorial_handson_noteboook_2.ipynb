{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc545a9e-05d6-4af1-a7ff-bef4dbbc1222",
   "metadata": {},
   "source": [
    "# TFM Tutorial Notebook 2: Tabular Foundation Models\n",
    "\n",
    "In this tutorial, we introduce a few cutting-edge foundational tabular models that leverage pretraining and in-context learning to achieve state-of-the-art performance on tabular datasets. These models represent a significant advancement in automated machine learning for structured data.\n",
    "\n",
    "Please use the following instructions to install AutoGluon if needed\n",
    "\n",
    "``!python -m pip install --upgrade pip``\n",
    "\n",
    "``!python -m pip install autogluon``\n",
    "\n",
    "Specifically, we will explore three foundational tabular models:\n",
    "\n",
    "1. **Mitra**-AutoGluon's new state-of-the-art tabular foundation model\n",
    "2. **TabPFN v2** - Prior-fitted networks for accurate predictions on small data\n",
    "3. **TabICL** - In-context learning for large tabular datasets\n",
    "\n",
    "*Note: In the lecture, we will introduce TabPFN. But the Prior Lab team has upgraded TabPFN to TabPFN v2, so we use TabPFN v2 here*\n",
    "\n",
    "In AutoGluon, the main tabular models are **Mitra** and **Chronos** (dedicated to time-series) to be introduced in the next module. Therefore, to use TabPFN and TabICL, you will need to install separately them through AutoGluon as follows:\n",
    "\n",
    "``!pip install uv``\n",
    "\n",
    "``!uv pip install autogluon.tabular[tabicl]``\n",
    "\n",
    "``!uv pip install autogluon.tabular[tabpfn]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41a881a-8dde-4f3c-8c49-cc316617c1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhjiang\\AppData\\Local\\miniconda3\\envs\\tabfm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_wine, fetch_california_housing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d9377-a1d1-43fd-8256-69058ce4a2cb",
   "metadata": {},
   "source": [
    "## Example Data\n",
    "\n",
    "For this tutorial, we will demonstrate the foundational models on three different datasets to showcase their versability:\n",
    "\n",
    "1. Wine Dataset (Multi-class Classification) - Medium-sized dataset for comparing model performance\n",
    "2. California Housing (Regression) - Regression dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4181ce0-8722-4b60-9e97-0050da46bb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "Wine: (178, 14)\n",
      "California Housing: (20640, 9)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "\n",
    "# 1. Wine (Multi-class Classification)\n",
    "wine_data = load_wine()\n",
    "wine_df = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)\n",
    "wine_df['target'] = wine_data.target\n",
    "\n",
    "# 2. California Housing (Regression)\n",
    "housing_data = fetch_california_housing()\n",
    "housing_df = pd.DataFrame(housing_data.data, columns=housing_data.feature_names)\n",
    "housing_df['target'] = housing_data.target\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Wine: {wine_df.shape}\")\n",
    "print(f\"California Housing: {housing_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fcaf9b-86ab-4868-8a02-ec4b6e3805b4",
   "metadata": {},
   "source": [
    "## Create Train/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27be4452-db4e-424d-80ea-9dceb524adb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set sizes:\n",
      "Wine: 142 samples\n",
      "Housing: 16512 samples\n"
     ]
    }
   ],
   "source": [
    "# Create train/test splits (80/20)\n",
    "wine_train, wine_test = train_test_split(wine_df, test_size=0.2, random_state=42, stratify=wine_df['target'])\n",
    "housing_train, housing_test = train_test_split(housing_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set sizes:\")\n",
    "print(f\"Wine: {len(wine_train)} samples\")\n",
    "print(f\"Housing: {len(housing_train)} samples\")\n",
    "\n",
    "# Convert to TabularDataset\n",
    "wine_train_data = TabularDataset(wine_train)\n",
    "wine_test_data = TabularDataset(wine_test)\n",
    "housing_train_data = TabularDataset(housing_train)\n",
    "housing_test_data = TabularDataset(housing_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e76ba-0f0b-4b8f-8193-5fa8768a4837",
   "metadata": {},
   "source": [
    "## 1. Mitra: AutoGluon's Tabular Foundation Model\n",
    "\n",
    "You can fit the model on your data with three lines of code. Built on the in-context learning paradigm and pretrained exclusively on synthetic data, Mitra introduces a principled pretraining approach by carefully selecting and mixing diverse synthetic priors to promote robust generalization across a wide range of real-world tabular datasets.\n",
    "\n",
    "**Mitra achieves state-of-the-art performance** on major benchmarks including TabRepo, TabZilla, AMLB, and TabArena, especially excelling on small tabular datasets with fewer than 5,000 samples and 100 features, for both classification and regression tasks.\n",
    "\n",
    "**Mitra supports both zero-shot and fine-tuning** modes and runs seamlessly on both GPU and CPU. Its weights are fully open-sourced under the Apache-2.0 license, making it a privacy-conscious and production-ready solution for enterprises concerned about data sharing and hosting.\n",
    "\n",
    "**Please make sure you have enough compute either on CPU or GPU for fitting the model (no gradient updates just in-context learning)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f40208e-e664-4f0d-81f2-154fd2e1dfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20251024_162506\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.11.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          8\n",
      "Memory Avail:       2.81 GB / 15.93 GB (17.6%)\n",
      "Disk Space Avail:   230.58 GB / 476.33 GB (48.4%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mitra classifier on classification dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"C:\\Users\\zhjiang\\Documents\\tabfm_tutorial\\AutogluonModels\\ag-20251024_162506\"\n",
      "Train Data Rows:    142\n",
      "Train Data Columns: 13\n",
      "Label Column:       target\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\t3 unique label values:  [np.int64(0), np.int64(2), np.int64(1)]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Train Data Class Count: 3\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2864.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 13 | ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', ...]\n",
      "\t0.0s = Fit runtime\n",
      "\t13 features in original data used to generate 13 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.01 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.06s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 113, Val Rows: 29\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'MITRA': [{'fine_tune': False}],\n",
      "}\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: Mitra ...\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 7.001 GB out of 2.669 GB available memory (262.270%)... (90.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=2.96 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train Mitra... Skipping this model.\n",
      "No base models to train on, skipping auxiliary stack level 2...\n",
      "Warning: AutoGluon did not successfully train any models\n",
      "AutoGluon training complete, total runtime = 4.22s ... Best model: None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No models were trained successfully during fit(). Inspect the log output or increase verbosity to determine why no models were fit. Alternatively, set `raise_on_no_models_fitted` to False during the fit call.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining Mitra classifier on classification dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m mitra_predictor = TabularPredictor(label=\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmitra_predictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwine_train_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMITRA\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfine_tune\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m   \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMitra training completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\miniconda3\\envs\\tabfm\\Lib\\site-packages\\autogluon\\core\\utils\\decorators.py:31\u001b[39m, in \u001b[36munpack.<locals>._unpack_inner.<locals>._call\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(f)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call\u001b[39m(*args, **kwargs):\n\u001b[32m     30\u001b[39m     gargs, gkwargs = g(*other_args, *args, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\miniconda3\\envs\\tabfm\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1363\u001b[39m, in \u001b[36mTabularPredictor.fit\u001b[39m\u001b[34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, fit_strategy, memory_limit, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1360\u001b[39m \u001b[38;5;66;03m# keep track of the fit strategy used for future calls\u001b[39;00m\n\u001b[32m   1361\u001b[39m \u001b[38;5;28mself\u001b[39m._fit_strategy = fit_strategy\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\miniconda3\\envs\\tabfm\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1371\u001b[39m, in \u001b[36mTabularPredictor._fit\u001b[39m\u001b[34m(self, ag_fit_kwargs, ag_post_fit_kwargs)\u001b[39m\n\u001b[32m   1369\u001b[39m \u001b[38;5;28mself\u001b[39m._learner.fit(**ag_fit_kwargs)\n\u001b[32m   1370\u001b[39m \u001b[38;5;28mself\u001b[39m._set_post_fit_vars()\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[38;5;28mself\u001b[39m.save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\miniconda3\\envs\\tabfm\\Lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:1703\u001b[39m, in \u001b[36mTabularPredictor._post_fit\u001b[39m\u001b[34m(self, keep_only_best, refit_full, set_best_to_refit_full, save_space, calibrate, calibrate_decision_threshold, infer_limit, num_cpus, num_gpus, refit_full_kwargs, fit_strategy, raise_on_no_models_fitted)\u001b[39m\n\u001b[32m   1701\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_names():\n\u001b[32m   1702\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_on_no_models_fitted:\n\u001b[32m-> \u001b[39m\u001b[32m1703\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1704\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mNo models were trained successfully during fit().\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1705\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m Inspect the log output or increase verbosity to determine why no models were fit.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1706\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m Alternatively, set `raise_on_no_models_fitted` to False during the fit call.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1707\u001b[39m         )\n\u001b[32m   1709\u001b[39m     logger.log(\u001b[32m30\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWarning: No models found, skipping post_fit logic...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: No models were trained successfully during fit(). Inspect the log output or increase verbosity to determine why no models were fit. Alternatively, set `raise_on_no_models_fitted` to False during the fit call."
     ]
    }
   ],
   "source": [
    "# Create predictor with Mitra\n",
    "print(\"Training Mitra classifier on classification dataset...\")\n",
    "mitra_predictor = TabularPredictor(label='target')\n",
    "mitra_predictor.fit(\n",
    "    wine_train_data,\n",
    "    hyperparameters={\n",
    "        'MITRA': {'fine_tune': False}\n",
    "    },\n",
    "   )\n",
    "\n",
    "print(\"\\nMitra training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69e3200-12c5-4bf5-926e-1bd1ad9614a8",
   "metadata": {},
   "source": [
    "## Evaluate Mitra Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd62a1e9-b27e-42c6-b927-127c2714d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "mitra_predictions = mitra_predictor.predict(wine_test_data)\n",
    "print(\"Sample Mitra predictions:\")\n",
    "print(mitra_predictions.head(10))\n",
    "\n",
    "# Show prediction probabilities for first few samples\n",
    "mitra_predictions = mitra_predictor.predict_proba(wine_test_data)\n",
    "print(mitra_predictions.head())\n",
    "\n",
    "# Show model leaderboard\n",
    "print(\"\\nMitra Model Leaderboard:\")\n",
    "mitra_predictor.leaderboard(wine_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81079f99-88ec-4907-8ba3-25f44972002d",
   "metadata": {},
   "source": [
    "## Fine-tuning with Mitra\n",
    "\n",
    "TFMs can be fine-tuned to improve the performance if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f120d36-a519-46b9-8347-a99e57d45489",
   "metadata": {},
   "outputs": [],
   "source": [
    "mitra_predictor_ft = TabularPredictor(label='target')\n",
    "mitra_predictor_ft.fit(\n",
    "    wine_train_data,\n",
    "    hyperparameters={\n",
    "        'MITRA': {'fine_tune': True, 'fine_tune_steps': 10}\n",
    "    },\n",
    "    time_limit=120,  # 2 minutes\n",
    "   )\n",
    "\n",
    "print(\"\\nMitra fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b833cafa-401d-4c75-93fc-6821e2bb41ae",
   "metadata": {},
   "source": [
    "## Evaluating Fine-tuned Mitra Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3e5db-27f5-4960-bf6e-c098fc76c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model leaderboard\n",
    "print(\"\\nMitra Model Leaderboard:\")\n",
    "mitra_predictor_ft.leaderboard(wine_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590612d-7827-489f-ac74-7da9610c9970",
   "metadata": {},
   "source": [
    "## Using Mitra for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca96f10-3fad-47a7-8336-45b1a73229a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictor with Mitra for regression\n",
    "print(\"Training Mitra regressor on California Housing dataset...\")\n",
    "mitra_reg_predictor = TabularPredictor(\n",
    "    label='target',\n",
    "    path='./mitra_regressor_model',\n",
    "    problem_type='regression'\n",
    ")\n",
    "mitra_reg_predictor.fit(\n",
    "    housing_train_data.sample(100), # sample 100 rows\n",
    "    hyperparameters={\n",
    "        'MITRA': {'fine_tune': False}\n",
    "    },\n",
    ")\n",
    "\n",
    "# Evaluate regression performance\n",
    "mitra_reg_predictor.leaderboard(housing_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61847f1-4a9d-4d2a-8238-30be330db84b",
   "metadata": {},
   "source": [
    "## 2. TabICL: In-Context Learning for Large-Scale Tabular Data\n",
    "\n",
    "TabICL is a foundation model dedicated to in-context learning on large tabular datasets.\n",
    "\n",
    "TabICL leverages transformer architecture with in-context learning capabilities, making it particularly effective for scenarios where you have limited training data but access to related examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b983567d-aff4-440c-b332-f622c67ed547",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv\n",
    "!uv pip install autogluon.tabular[tabicl]   # For TabICL\n",
    "# Train TabICL on dataset\n",
    "print(\"Training TabICL on wine dataset...\")\n",
    "tabicl_predictor = TabularPredictor(\n",
    "    label='target',\n",
    "    path='./tabicl_model'\n",
    ")\n",
    "tabicl_predictor.fit(\n",
    "    wine_train_data,\n",
    "    hyperparameters={\n",
    "        'TABICL': {},\n",
    "    },\n",
    ")\n",
    "\n",
    "# Show prediction probabilities for first few samples\n",
    "tabicl_predictions = tabicl_predictor.predict_proba(wine_test_data)\n",
    "print(tabicl_predictions.head())\n",
    "\n",
    "# Show TabICL leaderboard\n",
    "print(\"\\nTabICL Model Details:\")\n",
    "tabicl_predictor.leaderboard(wine_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b6cab-18dd-41f9-9c6b-1722352cec34",
   "metadata": {},
   "source": [
    "## 3. TabPFN v2: Prior-Fitted Networks\n",
    "\n",
    "TabPFNv2 is designed for accurate predictions on small tabular datasets by using prior-fitted network architectures.\n",
    "\n",
    "TabPFNv2 excels on small datasets (< 10,000 samples) by leveraging prior knowledge encoded in the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb9669-1aa6-4a3e-b49e-4612f37f5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install autogluon.tabular[tabpfn]   # For TabPFNv2\n",
    "# Train TabPFNv2 on Wine dataset (perfect size for TabPFNv2)\n",
    "print(\"Training TabPFNv2 on Wine dataset...\")\n",
    "tabpfnv2_predictor = TabularPredictor(\n",
    "    label='target',\n",
    "    path='./tabpfnv2_model'\n",
    ")\n",
    "tabpfnv2_predictor.fit(\n",
    "    wine_train_data,\n",
    "    hyperparameters={\n",
    "        'TABPFNV2': {\n",
    "            # TabPFNv2 works best with default parameters on small datasets\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "# Show prediction probabilities for first few samples\n",
    "tabpfnv2_predictions = tabpfnv2_predictor.predict_proba(wine_test_data)\n",
    "print(tabpfnv2_predictions.head())\n",
    "\n",
    "\n",
    "tabpfnv2_predictor.leaderboard(wine_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cc75f-bb80-450b-846c-24100a8b9308",
   "metadata": {},
   "source": [
    "## Advanced Usage: Combining Multiple Foundational Models\n",
    "\n",
    "AutoGluon allows you to combine multiple foundational models in a single predictor for enhanced performance through model stacking and ensembling.\n",
    "We also compare TFMs with traditional ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911504fa-c33c-4416-9a87-5957f1208573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure multiple foundational models together\n",
    "multi_foundation_config = {\n",
    "    'MITRA': {\n",
    "        'fine_tune': False,\n",
    "        # 'fine_tune_steps': 10\n",
    "    },\n",
    "    'TABPFNV2': {},\n",
    "    'TABICL': {},\n",
    "    'GBM': {}, # Gradient Boosting Machine\n",
    "    'NN_TORCH': {}, # Neural Network\n",
    "    \"CAT\": {}, # CatBoost\n",
    "    \"RF\": {}, # Random Forest\n",
    "    \"XGB\": {}, # XGBoost\n",
    "}\n",
    "\n",
    "print(\"Training ensemble of foundational models...\")\n",
    "ensemble_predictor = TabularPredictor(\n",
    "    label='target',\n",
    "    path='./ensemble_foundation_model' # For classification\n",
    "    # path='./mitra_regressor_model',      # For regression\n",
    "    # problem_type='regression' TabICL does not work for regression.\n",
    ").fit(\n",
    "    wine_train_data,\n",
    "    hyperparameters=multi_foundation_config,\n",
    "    # time_limit=300,  # More time for multiple models\n",
    ")\n",
    "\n",
    "# Evaluate ensemble performance\n",
    "ensemble_predictor.leaderboard(wine_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee84000-b8e4-48d7-bf51-581b4d4c2418",
   "metadata": {},
   "source": [
    "**For exercise: Please use ``load_iris`` and ``load_diabetes`` for classification and regression evaluation.**\n",
    "\n",
    "Please note that TabICL does not work for regression now at AutoGluon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58522b3d-e381-48b9-b3c4-2765d9c1312e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
